# ğŸš€ Fuel AI Data Pipeline â€” Full Rebuild (Phase 1 â†’ 4)

### Author
**Carrim-Jamal Browne**  
Cloud Infrastructure Engineer @ Fuel AI  
[LinkedIn](https://linkedin.com/in/carrim-browne) â€¢ [GitHub](https://github.com/cjbrowne18)

---

## ğŸ“˜ Overview
This repository documents the full rebuild of Fuel AIâ€™s dataset-transfer pipeline â€” from initial manual scripts to a scalable, monitored, and policy-compliant automation framework.

The pipeline moves data from **Google Drive (uploads)** â†’ **Google Cloud Storage (customer buckets)**, reducing manual effort, timeouts, and operational risk.

---

## ğŸ§­ Project Phases

| Phase | Focus | Status | Key Skills / Tools |
|:------|:------|:------:|:------------------|
| **1. Foundation** | Script-based Drive â†’ GCS transfer | âœ… Complete | rclone â€¢ gsutil â€¢ Bash â€¢ IAM â€¢ Logging |
| **2. Automation** | Schedule & trigger orchestration | ğŸŸ¡ In Progress | Cloud Run â€¢ Scheduler â€¢ Pub/Sub |
| **3. Observability** | Metrics, alerts & dashboards | ğŸ”œ Planned | Cloud Monitoring (MQL) â€¢ Alerts â€¢ Dashboards |
| **4. Hardening & Compliance** | Security, cost, & compliance optimization | ğŸ”œ Planned | NIST SP 800-53 â€¢ CIS Controls â€¢ Cost Insights |

---

## ğŸ§© Architecture Evolution

### Phase 1 â€” Script Foundation
Google Drive (Source)
â†“ (rclone)
Staging Bucket (GCS)
â†“ (gsutil -m cp)
Customer Bucket (Target)
â†“
Logs + Manifests (local)

- Modular bash scripts (each does one thing well)  
- Automatic folder creation & resumable transfers  
- Logging + manifest tracking for transparency  

### Phase 2 â€” Automation Layer
Drive Watcher / Cloud Scheduler
â†“ (trigger)
Cloud Run Container â†’ Transfer Workflow
â†“
Pub/Sub messages for each dataset
â†“
Run Phase 1 scripts inside managed jobs

- Fully automated â€œpush-button onceâ€ operation  
- Configurable scheduling per dataset  
- Audit logs & standardized exit codes  

### Phase 3 â€” Observability & Alerting
- Logs exported to GCS and Cloud Logging  
- Custom metrics (e.g., `transfer_success_count`, `transfer_error_count`)  
- Alert policies: failed runs, high egress, missing datasets  
- Daily status digest (Cloud Function + Scheduler)  

### Phase 4 â€” Security & Compliance Hardening
- Enforce UBLA + PAP on buckets  
- Service Account least-privilege model  
- Secret rotation + encryption via KMS  
- Compliance mapping (NIST, CIS, GDPR alignment)  
- Cost optimization via lifecycle policies + alerts  

---

## ğŸ—‚ Repository Structure
fuel-ai-data-pipeline/
â”œâ”€â”€ README.md
â”œâ”€â”€ scripts/
â”‚ â”œâ”€â”€ create_dest.sh
â”‚ â”œâ”€â”€ drive_to_gcs.sh
â”‚ â”œâ”€â”€ gcs_to_target.sh
â”‚ â””â”€â”€ run_dataset.sh
â”œâ”€â”€ automation/
â”‚ â”œâ”€â”€ scheduler_job.yaml
â”‚ â”œâ”€â”€ run_container.sh
â”‚ â””â”€â”€ pubsub_trigger.py
â”œâ”€â”€ monitoring/
â”‚ â”œâ”€â”€ alert_policies/
â”‚ â”œâ”€â”€ dashboards/
â”‚ â””â”€â”€ logs_based_metrics/
â”œâ”€â”€ security/
â”‚ â”œâ”€â”€ bucket_policies.yaml
â”‚ â””â”€â”€ iam_roles.tf
â”œâ”€â”€ design/
â”‚ â”œâ”€â”€ architecture_diagram.png
â”‚ â”œâ”€â”€ audits_summary.md
â”‚ â””â”€â”€ phase1_outline.pdf
â”œâ”€â”€ manifests/
â”œâ”€â”€ logs/
â””â”€â”€ CHANGELOG.md


---




